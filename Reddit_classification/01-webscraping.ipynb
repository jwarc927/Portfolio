{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4573609c-5079-4717-bd9f-bc923af19d3c",
   "metadata": {},
   "source": [
    "# Obtaining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf790b73-a502-4f9c-8e85-da28dfc5314a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ced039-09c8-48cf-b995-e08a44dba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9a3ab-f72b-4c7b-9642-16ce74090280",
   "metadata": {},
   "source": [
    "## Use Pushshift to Collect Data\n",
    "\n",
    "Here we define a function to scrape 100 reddit posts per loop, and provide a three second pause between iterations of the loop.  Once the json data of least 15,000 posts are collected, they are converted to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374067af-ccc5-4d62-9529-a3d504539913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jokes(subreddit, quantity):\n",
    "    posts = []\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    params = {\n",
    "    'subreddit' : subreddit,\n",
    "    'size' : 100,\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "\n",
    "    posts.extend(res.json()['data'])\n",
    "\n",
    "    while len(posts) < quantity:\n",
    "            time.sleep(3)\n",
    "            before_time = posts[-1]['created_utc']\n",
    "            params = {\n",
    "                'subreddit' : subreddit,\n",
    "                'size' : 100,\n",
    "                'before' : before_time\n",
    "                }\n",
    "            res = requests.get(url, params)\n",
    "            posts.extend(res.json()['data'])\n",
    "            print(len(posts))\n",
    "    return pd.DataFrame(posts)[['subreddit', 'title', 'selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bdf4d-5c87-483d-a5ae-fa72a77f673f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "onion = get_jokes('TheOnion', 15000)\n",
    "not_onion = get_jokes('nottheonion', 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a056b96-ee6d-4385-a1a4-ddb671f3f2ac",
   "metadata": {},
   "source": [
    "Next we save this data to a csv file for use in the next step of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96854b99-018a-4bec-8e1c-7658e09d7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onion.to_csv('./data/theonion.csv')\n",
    "not_onion.to_csv('./data/nottheonion.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
